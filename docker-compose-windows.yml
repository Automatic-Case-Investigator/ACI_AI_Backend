version: '3.5'

services:
  vllm:
    image: nvcr.io/nvidia/vllm:25.12.post1-py3
    container_name: vllm_gpt_oss
    ports:
      - "9000:8000"
    networks:
      - aci_backend_backend_net
    volumes:
      - model_volume:/root/.cache/huggingface
    entrypoint: ["vllm", "serve", "--model", "acezxn/ACI_Cyber_Base_GPT_OSS_20B", "--gpu-memory-utilization", "0.5"]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  redis:
    image: redis:latest
    healthcheck:
      test: [ "CMD-SHELL", "redis-cli ping | grep PONG" ]
      interval: 1s
      timeout: 3s
      retries: 5
    networks:
      - aci_backend_backend_net
    command: [ "redis-server" ]

  aci-ai-backend:
    build:
      context: .
      dockerfile: docker/backend/Dockerfile
    ports:
      - 8001:8000
    networks:
      - aci_backend_backend_net
    volumes:
      - model_volume:/app/models
      - db_volume:/app/database
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [ gpu ]
    depends_on:
      - redis
      - vllm

networks:
  # the same network as the network used by aci_backend
  aci_backend_backend_net:
    external: true

volumes:
  db_volume:
  model_volume:
