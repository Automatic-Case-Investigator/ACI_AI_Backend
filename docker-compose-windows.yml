version: '3.5'

services:
  ollama:
    image: ollama/ollama:latest
    pull_policy: always
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ./ollama:/model_files
    networks:
      - aci_backend_backend_net
    tty: true
    entrypoint: ["/bin/sh", "/model_files/run_ollama.sh"]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all 
              capabilities: [gpu]

  redis:
    image: redis:latest
    healthcheck:
      test: [ "CMD-SHELL", "redis-cli ping | grep PONG" ]
      interval: 1s
      timeout: 3s
      retries: 5
    networks:
      - aci_backend_backend_net
    command: [ "redis-server" ]

  aci-ai-backend:
    build:
      context: .
      dockerfile: docker/backend/Dockerfile
    ports:
      - 8001:8000
    networks:
      - aci_backend_backend_net
    volumes:
      - model_volume:/app/models
      - db_volume:/app/database
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [ gpu ]
    depends_on:
      - chroma
      - redis

networks:
  # the same network as the network used by aci_backend
  aci_backend_backend_net:
    external: true

volumes:
  chroma_data:
  db_volume:
  model_volume:
