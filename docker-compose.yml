version: '3.5'

services:  
  redis:
    image: redis:latest
    healthcheck:
      test: [ "CMD-SHELL", "redis-cli ping | grep PONG" ]
      interval: 1s
      timeout: 3s
      retries: 5
    command: [ "redis-server" ]

  vllm:
    image: nvcr.io/nvidia/vllm:25.12.post1-py3
    container_name: vllm_gpt_oss
    ports:
      - "9000:8000"
    volumes:
      - /opt/hf-cache:/root/.cache/huggingface
    entrypoint: ["vllm", "serve", "--model", "acezxn/ACI_Cyber_Base_GPT_OSS_20B", "--gpu-memory-utilization", "0.5"]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  aci-ai-backend:
    build:
      context: .
      dockerfile: docker/backend/Dockerfile
    ports:
      - 8001:8000
    volumes:
      - model_volume:/app/models
      - db_volume:/app/database
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [ gpu ]
    depends_on:
      - redis
      - vllm

volumes:
  db_volume:
  model_volume:
